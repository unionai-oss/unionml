{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa6f98e00f2534739354f89caabea908",
   "metadata": {},
   "source": [
    "# QuickDraw: A Pictionary App\n",
    "\n",
    "In this example, we'll see how to create pictionary app that uses the\n",
    "[QuickDraw](https://github.com/googlecreativelab/quickdraw-dataset) dataset\n",
    "to train a convolutional neural net to predict the semantic label of a\n",
    "hand-drawn picture.\n",
    "\n",
    "We'll break this tutorial up into two parts:\n",
    "\n",
    "1. Creating plain Python classes and functions to implement the quickdraw dataset\n",
    "   and model using [`pytorch`](https://pytorch.org/) and the Hugging Face\n",
    "   [`transformers`](https://huggingface.co/docs/transformers/index) library.\n",
    "2. Using the pieces in part 1 to create a UnionML app for training a model\n",
    "   and serving predictions using a [`gradio`](https://gradio.app/) widget.\n",
    "\n",
    "## Part 1: Implementing the Quickdraw Model\n",
    "\n",
    "```{note}\n",
    "This tutorial is adapted from this [gradio guide](https://gradio.app/building_a_pictionary_app/),\n",
    "and you can find the original notebook [here](https://github.com/nateraw/quickdraw-pytorch).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeddd9aeb703cc6ae78976635f60c4b",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install gradio numpy tqdm requests torch transformers unionml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df810ed24caf23d5b6f1f8d74cc40db",
   "metadata": {},
   "source": [
    "First let's import everything we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae7244914163838011bdf2c433b8310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Optional\n",
    "\n",
    "import urllib.request\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a776d1b8b5785566f5ccbc3d56022d4",
   "metadata": {},
   "source": [
    "Then let's implement some helper functions for downloading the quickdraw data and loading it\n",
    "into memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bdde97e06ffac78b6ad5297bf6e30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES_URL = \"https://raw.githubusercontent.com/googlecreativelab/quickdraw-dataset/master/categories.txt\"\n",
    "DATASET_URL = \"https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/\"\n",
    "\n",
    "\n",
    "def get_quickdraw_class_names():\n",
    "    \"\"\"Get the class names associated with the quickdraw dataset.\"\"\"\n",
    "    return [*sorted(x.replace(' ', '_') for x in requests.get(CLASSES_URL).text.splitlines())]\n",
    "\n",
    "\n",
    "def download_quickdraw_dataset(\n",
    "    root: str = \"./data\",\n",
    "    limit: Optional[int] = None,\n",
    "    class_names: List[str]=None,\n",
    "):\n",
    "    \"\"\"Download quickdraw data to a directory containing files for each class label.\"\"\"\n",
    "    class_names = class_names or get_quickdraw_class_names()\n",
    "    root = Path(root)\n",
    "    root.mkdir(exist_ok=True, parents=True)\n",
    "    print(\"Downloading Quickdraw Dataset...\")\n",
    "    for class_name in tqdm(class_names[:limit]):\n",
    "        urllib.request.urlretrieve(\n",
    "            f\"{DATASET_URL}{class_name.replace('_', '%20')}.npy\",\n",
    "            root / f\"{class_name}.npy\"\n",
    "        )\n",
    "\n",
    "\n",
    "def load_quickdraw_data(root: str = \"./data\", max_items_per_class: int = 5000):\n",
    "    \"\"\"Load quickdraw data in to memory, returning features, labels, and class names.\"\"\"\n",
    "    x = np.empty([0, 784], dtype=np.uint8)\n",
    "    y = np.empty([0], dtype=np.long)\n",
    "    class_names = []\n",
    "    print(f\"Loading {max_items_per_class} examples for each class from the Quickdraw Dataset...\")\n",
    "    for idx, file in enumerate(tqdm(sorted(Path(root).glob('*.npy')))):\n",
    "        data = np.load(file, mmap_mode='r')[0: max_items_per_class, :]\n",
    "        x = np.concatenate((x, data), axis=0)\n",
    "        y = np.append(y, np.full(data.shape[0], idx))\n",
    "        class_names.append(file.stem)\n",
    "    return x, y, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad069c9e675ad26223488c4d5f2bd94",
   "metadata": {},
   "source": [
    "### QuickDraw Dataset\n",
    "\n",
    "Next we implement the `QuickDrawDataset` using `torch.utils.data.Dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cc0d90b2064cef307833d53faec0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuickDrawDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, max_items_per_class=5000, class_limit=None):\n",
    "        super().__init__()\n",
    "        self.root = root\n",
    "        self.max_items_per_class = max_items_per_class\n",
    "        self.class_limit = class_limit\n",
    "\n",
    "        download_quickdraw_dataset(self.root, self.class_limit)\n",
    "        self.X, self.Y, self.classes = load_quickdraw_data(\n",
    "            self.root, self.max_items_per_class,\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = (self.X[idx] / 255.).astype(np.float32).reshape(1, 28, 28)\n",
    "        y = self.Y[idx]\n",
    "        return torch.from_numpy(x), y.item()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        return {\n",
    "            'pixel_values': torch.stack([item[0] for item in batch]),\n",
    "            'labels': torch.LongTensor([item[1] for item in batch]),\n",
    "        }\n",
    "\n",
    "    def split(self, pct=0.1):\n",
    "        indices = torch.randperm(len(self)).tolist()\n",
    "        n_val = math.floor(len(indices) * pct)\n",
    "        train_ds = torch.utils.data.Subset(self, indices[:-n_val])\n",
    "        val_ds = torch.utils.data.Subset(self, indices[-n_val:])\n",
    "        return train_ds, val_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af9e415d42aeae24d37f614e6a0a722",
   "metadata": {},
   "source": [
    "As you'll see later, this class is important so that the `transformers` library can\n",
    "handle the automatic batching of data during training.\n",
    "\n",
    "### QuickDraw Model and Trainer\n",
    "\n",
    "Now let's define the model architecture for our ConvNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55339d9ed03a230487ad15903480051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "def init_model(num_classes: int) -> nn.Module:\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(1, 64, 3, padding='same'),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.Conv2d(64, 128, 3, padding='same'),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.Conv2d(128, 256, 3, padding='same'),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(2304, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, num_classes),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aef8254b4f85fc573b0d2634d1031cf",
   "metadata": {},
   "source": [
    "As you can see it's a fairly straightforward 2D ConvNet architecture that uses a square\n",
    "kernel size of 3, Relu layers for its non-linear activation operator, and max-pooling.\n",
    "\n",
    "Next, let's create a subclass of `transformers.Trainer` to implement a custom loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1f737b000388428c8fe260b045a712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EvalPrediction, Trainer, TrainingArguments\n",
    "from transformers.modeling_utils import ModelOutput\n",
    "\n",
    "\n",
    "class QuickDrawTrainer(Trainer):\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        logits, labels = model(inputs[\"pixel_values\"]), inputs.get(\"labels\")\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = torch.nn.CrossEntropyLoss()(logits, labels)\n",
    "        return (loss, ModelOutput(logits=logits, loss=loss)) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90c098b10d15d044f6cea8985a07b5f",
   "metadata": {},
   "source": [
    "Then, let's define helper functions to compute the accuracy metric, which will be how we'll\n",
    "judge the performance of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4feaa1956c216a93384fbc10ef5a38ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from: https://github.com/rwightman/pytorch-image-models/blob/master/timm/utils/metrics.py\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k.\"\"\"\n",
    "    maxk = min(max(topk), output.size()[1])\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n",
    "    return [correct[:min(k, maxk)].reshape(-1).float().sum(0) * 100. / batch_size for k in topk]\n",
    "\n",
    "def quickdraw_compute_metrics(p: EvalPrediction):\n",
    "    if p.label_ids is None:\n",
    "        return {}\n",
    "    acc1, acc5 = accuracy(p.predictions, p.label_ids, topk=(1, 5))\n",
    "    return {'acc1': acc1, 'acc5': acc5}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155892fabbb841692103aa27800ef9aa",
   "metadata": {},
   "source": [
    "Finally, let's create a `train_quickdraw` function that will serve as the main entrypoint\n",
    "for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1126fd6f736790418a06915319631a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def train_quickdraw(module: nn.Module, dataset: QuickDrawDataset, num_epochs: int, batch_size: int):\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d-%H%M%S')\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'~/.tmp/outputs_20k_{timestamp}',\n",
    "        save_strategy='epoch',\n",
    "        report_to=['tensorboard'],\n",
    "        logging_strategy='steps',\n",
    "        logging_steps=100,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        learning_rate=0.003,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        dataloader_drop_last=True,\n",
    "        num_train_epochs=num_epochs,\n",
    "        warmup_steps=10000,\n",
    "        save_total_limit=5,\n",
    "    )\n",
    "\n",
    "    print(f\"Training on device: {training_args.device}\")\n",
    "\n",
    "    quickdraw_trainer = QuickDrawTrainer(\n",
    "        module,\n",
    "        training_args,\n",
    "        data_collator=dataset.collate_fn,\n",
    "        train_dataset=dataset,\n",
    "        tokenizer=None,\n",
    "        compute_metrics=quickdraw_compute_metrics,\n",
    "    )\n",
    "    train_results = quickdraw_trainer.train()\n",
    "    quickdraw_trainer.save_model()\n",
    "    quickdraw_trainer.log_metrics(\"train\", train_results.metrics)\n",
    "    quickdraw_trainer.save_metrics(\"train\", train_results.metrics)\n",
    "    quickdraw_trainer.save_state()\n",
    "    return module\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682e5168e5c25cfbef195afa04a1b8d9",
   "metadata": {},
   "source": [
    "Why did we do through all of this trouble to implementing the dataset and model classes/functions\n",
    "instead of embedding it inside our UnionML app?\n",
    "\n",
    "Well, it often makes sense to separate the concerns of the dataset/model implementation from the\n",
    "application code that will scale or serve it, especially for more complex projects. Depending on the\n",
    "the complexity of the data processing and modeling logic needed to train your model, you may want to\n",
    "create separate functions/classes/modules to abstract it away.\n",
    "\n",
    "In the next section, we'll see that this pays dividends in terms of readability and maintainability.\n",
    "\n",
    "## Part 2: Creating a UnionML Pictionary App\n",
    "\n",
    "Now that we have all the pieces we need to train our model, let's create the UnionML app. First we\n",
    "import what we need and define our `unionml.Dataset` and `unionml.Model` objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e162a9be67bd248b57ab1917dde517c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import EvalPrediction\n",
    "from unionml import Dataset, Model\n",
    "\n",
    "dataset = Dataset(name=\"quickdraw_dataset\", test_size=0.2, shuffle=True)\n",
    "model = Model(name=\"quickdraw_classifier\", init=init_model, dataset=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c41efb29d10630aa779d3876865680",
   "metadata": {},
   "source": [
    "### Reading the Dataset\n",
    "\n",
    "Then, we implement the `reader` function, which returns a `QuickDrawDataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0594f75e24d96fe35a46a9183e70a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataset.reader(cache=True, cache_version=\"1\")\n",
    "def reader(\n",
    "    data_dir: str, max_examples_per_class: int = 1000, class_limit: int = 5\n",
    ") -> QuickDrawDataset:\n",
    "    return QuickDrawDataset(data_dir, max_examples_per_class, class_limit=class_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43874e12a35b72bc8f114390c2daf0f",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Next, we define the `trainer` function, using the `quickdraw_trainer` helper function we\n",
    "defined above and an `evaluator` function to let UnionML know how to evaluate the model\n",
    "on some partition of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9bef2fb10e403872e3da59cca39722",
   "metadata": {},
   "outputs": [],
   "source": [
    "@model.trainer(cache=True, cache_version=\"1\")\n",
    "def trainer(\n",
    "    module: nn.Module,\n",
    "    dataset: torch.utils.data.Subset,\n",
    "    *,\n",
    "    num_epochs: int = 20,\n",
    "    batch_size: int = 256,\n",
    ") -> nn.Module:\n",
    "    return train_quickdraw(module, dataset, num_epochs, batch_size)\n",
    "\n",
    "@model.evaluator\n",
    "def evaluator(module: nn.Module, dataset: QuickDrawDataset) -> float:\n",
    "    cuda = torch.cuda.is_available()\n",
    "    module = module.cuda() if cuda else module\n",
    "    acc = []\n",
    "    for features, label_ids in torch.utils.data.DataLoader(dataset, batch_size=256):\n",
    "        features = features.to(\"cuda\") if cuda else features\n",
    "        label_ids = label_ids.to(\"cuda\") if cuda else label_ids\n",
    "        metrics = quickdraw_compute_metrics(EvalPrediction(module(features), label_ids))\n",
    "        acc.append(metrics[\"acc1\"])\n",
    "    module.cpu()\n",
    "    return float(sum(acc) / len(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c38501ce8050fe40bcb48f12e90a513",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "Because we expect to generate predictions from raw images in the form of a numpy array,\n",
    "we need to register a `feature_loader` function in the `dataset` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beabaf182024096b79136c5c305706dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataset.feature_loader\n",
    "def feature_loader(data: np.ndarray) -> torch.Tensor:\n",
    "    return torch.tensor(data, dtype=torch.float32).unsqueeze(0).unsqueeze(0) / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4187c2783a4c7c15d8aa8284ead1516",
   "metadata": {},
   "source": [
    "Then we can define a `predictor` function that consumes the output of `feature_loader`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3f95b6ce8729fde44eed02fab18084",
   "metadata": {},
   "outputs": [],
   "source": [
    "@model.predictor(cache=True, cache_version=\"1\")\n",
    "def predictor(module: nn.Module, features: torch.Tensor) -> dict:\n",
    "    module.eval()\n",
    "    if torch.cuda.is_available():\n",
    "        module, features = module.cuda(), features.cuda()\n",
    "    with torch.no_grad():\n",
    "        probabilities = nn.functional.softmax(module(features)[0], dim=0)\n",
    "    class_names = get_quickdraw_class_names()\n",
    "    values, indices = torch.topk(probabilities, 3)\n",
    "    return {class_names[i]: v.item() for i, v in zip(indices, values)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4000b9c0a21b489fa9ca1ab8331e62d1",
   "metadata": {},
   "source": [
    "### Training a Model Locally\n",
    "\n",
    "Awesome! If you've been following along in your editor or a Jupyter notebook, you just implemented\n",
    "a pictionary app in UnionML ‚≠êÔ∏è\n",
    "\n",
    "Now let's train a model just using 10 classes, with 500 examples per class, for 1 epoch. This\n",
    "model won't perform that well, so feel free to change these numbers up in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fd2b912b3cce6cbe6c91be9c2b22c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "max_examples_per_class = 500\n",
    "num_epochs = 1\n",
    "batch_size = 256\n",
    "\n",
    "model.train(\n",
    "    hyperparameters={\"num_classes\": num_classes},\n",
    "    trainer_kwargs={\"num_epochs\": num_epochs, \"batch_size\": batch_size},\n",
    "    data_dir=\"/tmp/quickdraw_data\",\n",
    "    max_examples_per_class=max_examples_per_class,\n",
    "    class_limit=num_classes,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61660d339fb4a5b42076a431804bb344",
   "metadata": {},
   "source": [
    "### Serving on a Gradio Widget\n",
    "\n",
    "And now the moment of truth üôå\n",
    "\n",
    "To create a `gradio` widget, we can simply use the `model.predict` method into the\n",
    "`gradio.Interface` object using a `lambda` function to handle the `None` case when we press\n",
    "the `clear` button on the widget:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1234b020c250b6a2815167cacb513b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "gr.Interface(\n",
    "    fn=lambda img: img if img is None else model.predict(img),\n",
    "    inputs=\"sketchpad\",\n",
    "    outputs=\"label\",\n",
    "    live=True,\n",
    "    allow_flagging=\"never\",\n",
    ").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
